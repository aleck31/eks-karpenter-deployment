# GPU NodePool 配置
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: nodepool-gpu
spec:
  # 模板配置
  template:
    metadata:
      labels:
        node-type: gpu
        workload-type: ml
    spec:
      # 节点类别配置
      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: nodeclass-gpu
      
      # 污点配置 - 确保只有 GPU 工作负载调度到这些节点
      taints:
        - key: nvidia.com/gpu
          value: "true"
          effect: NoSchedule
      
      # 资源要求
      requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["spot", "on-demand"]
        - key: node.kubernetes.io/instance-type
          operator: In
          values: 
            # G4dn 系列 - NVIDIA T4 (x86_64) - 小模型推理
            - g4dn.xlarge    # 1x T4, 4 vCPU, 16 GB, 125GB NVMe
            - g4dn.2xlarge   # 1x T4, 8 vCPU, 32 GB, 225GB NVMe  
            - g4dn.4xlarge   # 1x T4, 16 vCPU, 64 GB, 225GB NVMe
            - g4dn.8xlarge   # 1x T4, 32 vCPU, 128 GB, 900GB NVMe
            - g4dn.12xlarge  # 4x T4, 48 vCPU, 192 GB, 900GB NVMe
            
            # G6 系列 - NVIDIA L4 (x86_64) - 高效推理
            - g6.xlarge      # 1x L4, 4 vCPU, 16 GB
            - g6.2xlarge     # 1x L4, 8 vCPU, 32 GB
            - g6.4xlarge     # 1x L4, 16 vCPU, 64 GB
            - g6.8xlarge     # 1x L4, 32 vCPU, 128 GB
            - g6.12xlarge    # 4x L4, 48 vCPU, 192 GB

            # G5g 系列 - NVIDIA T4g (ARM64) - 成本优化推理
            - g5g.xlarge     # 1x T4g, 4 vCPU, 8 GB
            - g5g.2xlarge    # 1x T4g, 8 vCPU, 16 GB
            - g5g.4xlarge    # 1x T4g, 16 vCPU, 32 GB
            - g5g.8xlarge    # 1x T4g, 32 vCPU, 64 GB
            - g5g.16xlarge   # 2x T4g, 64 vCPU, 128 GB
            
            # G5 系列 - NVIDIA A10G (x86_64) - 中大模型推理
            - g5.xlarge      # 1x A10G, 4 vCPU, 16 GB
            - g5.2xlarge     # 1x A10G, 8 vCPU, 32 GB
            - g5.4xlarge     # 1x A10G, 16 vCPU, 64 GB
            - g5.8xlarge     # 1x A10G, 32 vCPU, 128 GB
            - g5.12xlarge    # 4x A10G, 48 vCPU, 192 GB
            
            # G6e 系列 - NVIDIA L40S (x86_64) - 大模型推理
            - g6e.xlarge     # 1x L40S, 4 vCPU, 16 GB
            - g6e.2xlarge    # 1x L40S, 8 vCPU, 32 GB
            - g6e.4xlarge    # 1x L40S, 16 vCPU, 64 GB
            - g6e.8xlarge    # 1x L40S, 32 vCPU, 128 GB
            - g6e.12xlarge   # 4x L40S, 48 vCPU, 192 GB

        - key: kubernetes.io/arch
          operator: In
          values: ["amd64", "arm64"]  # 支持 x86_64 和 ARM64
  
  # 扩展限制
  limits:
    cpu: 1000
    memory: 1000Gi
  
  # 节点终止策略
  disruption:
    # 生产环境建议设为: WhenEmpty 以避免频繁迁移
    consolidationPolicy: WhenEmptyOrUnderutilized
    consolidateAfter: 30m

---
apiVersion: karpenter.k8s.aws/v1
kind: EC2NodeClass
metadata:
  name: nodeclass-gpu
spec:
  # AMI 配置 - 使用 AL2023 EKS-optimized NVIDIA AMI
  amiFamily: AL2023
  amiSelectorTerms:
    - name: "amazon-eks-node-al2023-x86_64-nvidia-1.33-*"
      owner: "amazon"
  
  # 实例配置 - 本地 NVMe 存储策略
  instanceStorePolicy: RAID0  # 仅对本地 NVMe 设备做 RAID0
  
  # 用户数据 - AL2023 优化配置 + 本地存储挂载
  userData: |
    #!/bin/bash
    /usr/bin/nodeadm init --cluster-name eks-karpenter-env
    
    # 确保 kubelet 服务启动
    systemctl enable kubelet
    systemctl start kubelet
    
    # Configure local NVMe storage mount point
    mkdir -p /mnt/nvme-cache
    
    # Check for local NVMe storage (RAID0 configured by Karpenter)
    if [ -b /dev/nvme1n1 ]; then
        # Format and mount local storage for cache
        mkfs.ext4 /dev/nvme1n1
        mount /dev/nvme1n1 /mnt/nvme-cache
        chmod 777 /mnt/nvme-cache
        
        # Add to fstab for auto-mount after reboot
        echo "/dev/nvme1n1 /mnt/nvme-cache ext4 defaults,nofail 0 2" >> /etc/fstab
        
        # Create Kubernetes local storage directory
        mkdir -p /mnt/nvme-cache/k8s-local-storage
        chmod 777 /mnt/nvme-cache/k8s-local-storage
    fi
  
  # 安全组和子网
  securityGroupSelectorTerms:
    - tags:
        karpenter.sh/discovery: "eks-karpenter-env"
  subnetSelectorTerms:
    - tags:
        karpenter.sh/discovery: "eks-karpenter-env"
  
  # IAM 角色
  role: "KarpenterNodeInstanceRole-eks-karpenter-env"

  metadataOptions:
    httpPutResponseHopLimit: 2    # 允许容器访问IMDS
    
  # 块设备映射 - 为 ML 工作负载提供足够存储
  blockDeviceMappings:
    - deviceName: /dev/xvda
      ebs:
        volumeSize: 100Gi  # 持久化卷存储空间
        volumeType: gp3
        iops: 3000
        throughput: 125
        deleteOnTermination: true
