# EKS GPU æ”¯æŒéƒ¨ç½²æŒ‡å—

## æ¦‚è¿°

æœ¬æŒ‡å—å¸®åŠ©åœ¨ç°æœ‰ EKS + Karpenter é›†ç¾¤åŸºç¡€ä¸Šæ·»åŠ  GPU æ”¯æŒï¼Œç”¨äºæœºå™¨å­¦ä¹ å·¥ä½œè´Ÿè½½ã€‚

## ğŸ¯ GPU æ¶æ„è®¾è®¡

### æŠ€æœ¯æ ˆ
- **AMI**: Deep Learning OSS Nvidia Driver AMI (AL2023)
- **GPU å®ä¾‹**: G4dn, G5, G6, G6e, P3, P4 ç­‰ç³»åˆ—
- **å®¹å™¨è¿è¡Œæ—¶**: containerd + nvidia-container-runtime
- **è®¾å¤‡æ’ä»¶**: NVIDIA Device Plugin
- **è°ƒåº¦ç­–ç•¥**: Taint/Toleration + NodeSelector

### æ¶æ„åŸåˆ™
1. **ä¸“ç”¨èŠ‚ç‚¹æ± ** - GPU èŠ‚ç‚¹ç‹¬ç«‹ç®¡ç†
2. **æ±¡ç‚¹éš”ç¦»** - é˜²æ­¢é GPU å·¥ä½œè´Ÿè½½è°ƒåº¦
3. **æˆæœ¬ä¼˜åŒ–** - æ”¯æŒ Spot å®ä¾‹
4. **è‡ªåŠ¨æ‰©ç¼©å®¹** - åŸºäºå·¥ä½œè´Ÿè½½éœ€æ±‚

## ğŸš€ éƒ¨ç½²æ­¥éª¤

### 1. éƒ¨ç½² GPU NodePool

GPU NodePool æ¨èä½¿ç”¨ EKS ä¼˜åŒ–çš„ NVIDIA AMIï¼Œé¢„é›†æˆäº† GPU æ”¯æŒæ‰€éœ€çš„æ ¸å¿ƒç»„ä»¶ã€‚

**AL2023 EKS-optimized NVIDIA AMI** åŒ…å«ä»¥ä¸‹é¢„é…ç½®ç»„ä»¶ï¼š

| ç»„ä»¶ | ç‰ˆæœ¬ | åŠŸèƒ½ | çŠ¶æ€ |
|------|------|------|------|
| **NVIDIA é©±åŠ¨** | 470+ | GPU ç¡¬ä»¶é©±åŠ¨ç¨‹åº | âœ… é¢„è£… |
| **CUDA è¿è¡Œæ—¶** | 11.4+ | GPU è®¡ç®—åº“å’Œå·¥å…· | âœ… é¢„è£… |
| **Container Runtime** | containerd + nvidia-runtime | å®¹å™¨ GPU è®¿é—®æ”¯æŒ | âœ… é¢„é…ç½® |
| **kubelet GPU æ”¯æŒ** | - | GPU èµ„æºè¯†åˆ«å’Œç®¡ç† | âœ… é¢„é…ç½® |
| **NVIDIA Device Plugin** | - | Kubernetes GPU èµ„æºè°ƒåº¦ | âŒ éœ€æ‰‹åŠ¨éƒ¨ç½² |

#### éƒ¨ç½² NodePool

```bash
# åº”ç”¨ GPU NodePool é…ç½®
kubectl apply -f gpu/nodepool-gpu.yaml

# éªŒè¯ NodePool åˆ›å»º
kubectl get nodepool nodepool-gpu
kubectl get ec2nodeclass nodeclass-gpu
```

**NodePool é…ç½®è¦ç‚¹**ï¼š
- **AMI é€‰æ‹©**: `amazon-eks-node-al2023-x86_64-nvidia-*` 
- **å®ä¾‹å­˜å‚¨**: `instanceStorePolicy: RAID0` (æœ¬åœ° NVMe ä¼˜åŒ–)
- **ç”¨æˆ·æ•°æ®**: åŒ…å« kubelet å¯åŠ¨ä¿®å¤å’Œå­˜å‚¨æŒ‚è½½é…ç½®

### 2. éƒ¨ç½² NVIDIA Device Plugin

NVIDIA Device Plugin è´Ÿè´£ GPU èµ„æºçš„å‘ç°ã€åˆ†é…å’Œç®¡ç†ï¼Œæ˜¯ç”Ÿäº§ç¯å¢ƒçš„å¿…éœ€ç»„ä»¶ã€‚
**Device Plugin åŠŸèƒ½**:
- **GPU èµ„æºå‘ç°**: è‡ªåŠ¨è¯†åˆ«èŠ‚ç‚¹ä¸Šçš„ GPU è®¾å¤‡
- **èµ„æºå¹¿å‘Š**: å‘ Kubernetes API æŠ¥å‘Š `nvidia.com/gpu` èµ„æº
- **è®¾å¤‡åˆ†é…**: ä¸º Pod åˆ†é…ä¸“ç”¨ GPU è®¾å¤‡
- **èµ„æºéš”ç¦»**: é˜²æ­¢å¤šä¸ª Pod äº‰æŠ¢åŒä¸€ GPU

**ä¸ºä»€ä¹ˆéœ€è¦ Device Pluginï¼Ÿ**

è™½ç„¶ EKS GPU AMI é¢„è£…äº† GPU é©±åŠ¨å’Œè¿è¡Œæ—¶ï¼Œä½† Kubernetes å±‚é¢çš„ GPU èµ„æºç®¡ç†éœ€è¦é¢å¤–çš„ Device Pluginï¼š

| åŠŸèƒ½ | EKS GPU AMI | NVIDIA Device Plugin |
|------|-------------|---------------------|
| GPU ç¡¬ä»¶è®¿é—® | âœ… æ”¯æŒ | - |
| å®¹å™¨ GPU è¿è¡Œ | âœ… æ”¯æŒ | - |
| GPU èµ„æºè°ƒåº¦ | âŒ ä¸æ”¯æŒ | âœ… æä¾› |
| èµ„æºéš”ç¦»ç®¡ç† | âŒ ä¸æ”¯æŒ | âœ… æä¾› |

#### éƒ¨ç½²æ­¥éª¤
```bash
kubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.16.2/deployments/static/nvidia-device-plugin.yml
```

#### éªŒè¯ Device Plugin éƒ¨ç½²çŠ¶æ€
```bash
kubectl get daemonset -n kube-system nvidia-device-plugin-daemonset

# output:
NAME                             DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
nvidia-device-plugin-daemonset   1         1         1       1            1           <none>          2m
```

#### æ£€æŸ¥ Device Plugin Pod è¿è¡ŒçŠ¶æ€
```bash
kubectl get pods -n kube-system -l name=nvidia-device-plugin-ds

# output:
NAME                                   READY   STATUS    RESTARTS   AGE
nvidia-device-plugin-daemonset-xxxxx   1/1     Running   0          2m
```

### 3. éªŒè¯ GPU èŠ‚ç‚¹

```bash
# ç­‰å¾… GPU èŠ‚ç‚¹å¯åŠ¨ (å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)
kubectl get nodes -l node-type=gpu

# æ£€æŸ¥èŠ‚ç‚¹ GPU èµ„æº
kubectl describe node <gpu-node-name> | grep nvidia.com/gpu
```

## ğŸ§ª æµ‹è¯• GPU åŠŸèƒ½

### 1. åŸºç¡€ GPU æ£€æµ‹ (å¿«é€ŸéªŒè¯)

```bash
# éƒ¨ç½²åŸºç¡€ GPU æµ‹è¯• - nvidia-smi æ£€æµ‹
kubectl apply -f tests/test-gpu-simple.yaml

# æŸ¥çœ‹æµ‹è¯•ç»“æœ
kubectl logs gpu-simple-test
```

**é¢„æœŸè¾“å‡º**ï¼š
```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.182.03   Driver Version: 470.182.03   CUDA Version: 11.4   |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla T4            Off  | 00000000:00:1E.0 Off |                    0 |
| N/A   34C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
```

### 2. PyTorch GPU åŠŸèƒ½æµ‹è¯• (å®Œæ•´éªŒè¯)

```bash
# éƒ¨ç½² PyTorch GPU æµ‹è¯• - å®Œæ•´ ML æ¡†æ¶éªŒè¯
kubectl apply -f tests/test-gpu-pytorch.yaml

# æŸ¥çœ‹æµ‹è¯•ç»“æœ (å®¹å™¨ä¼šæŒç»­è¿è¡Œ)
kubectl logs gpu-pytorch-test

# æˆ–ç›´æ¥åœ¨å®¹å™¨å†…æ‰§è¡Œæµ‹è¯•
kubectl exec gpu-pytorch-test -- python3 -c "
import torch
print(f'CUDA available: {torch.cuda.is_available()}')
print(f'GPU name: {torch.cuda.get_device_name(0)}')
"
```

### 3. GPU + æœ¬åœ°å­˜å‚¨æµ‹è¯• (å­˜å‚¨é›†æˆ)

```bash
# éƒ¨ç½² GPU + NVMe å­˜å‚¨æµ‹è¯•
kubectl apply -f tests/test-gpu-nvme.yaml

# æŸ¥çœ‹å­˜å‚¨æµ‹è¯•ç»“æœ
kubectl logs gpu-storage-test
```

## ğŸ“‹ GPU å®ä¾‹ç±»å‹é€‰æ‹©

### G4dn ç³»åˆ— (NVIDIA T4)
- **é€‚ç”¨åœºæ™¯**: æ¨ç†ã€è½»é‡è®­ç»ƒ
- **æ€§ä»·æ¯”**: é«˜
- **æ¨èç”¨é€”**: æ¨¡å‹æ¨ç†ã€å¼€å‘æµ‹è¯•

| å®ä¾‹ç±»å‹ | GPU | vCPU | å†…å­˜ | ç½‘ç»œæ€§èƒ½ |
|---------|-----|------|------|----------|
| g4dn.xlarge | 1x T4 | 4 | 16 GB | æœ€é«˜ 25 Gbps |
| g4dn.2xlarge | 1x T4 | 8 | 32 GB | æœ€é«˜ 25 Gbps |
| g4dn.4xlarge | 1x T4 | 16 | 64 GB | æœ€é«˜ 25 Gbps |

### G5 ç³»åˆ— (NVIDIA A10G)
- **é€‚ç”¨åœºæ™¯**: è®­ç»ƒã€æ¨ç†ã€å›¾å½¢å·¥ä½œè´Ÿè½½
- **æ€§èƒ½**: æ¯” T4 é«˜ 2.5x
- **æ¨èç”¨é€”**: ä¸­ç­‰è§„æ¨¡è®­ç»ƒã€é«˜æ€§èƒ½æ¨ç†

| å®ä¾‹ç±»å‹ | GPU | vCPU | å†…å­˜ | ç½‘ç»œæ€§èƒ½ |
|---------|-----|------|------|----------|
| g5.xlarge | 1x A10G | 4 | 16 GB | æœ€é«˜ 10 Gbps |
| g5.2xlarge | 1x A10G | 8 | 32 GB | æœ€é«˜ 10 Gbps |
| g5.4xlarge | 1x A10G | 16 | 64 GB | æœ€é«˜ 25 Gbps |

### P3 ç³»åˆ— (NVIDIA V100)
- **é€‚ç”¨åœºæ™¯**: å¤§è§„æ¨¡è®­ç»ƒã€HPC
- **æ€§èƒ½**: æœ€é«˜
- **æ¨èç”¨é€”**: æ·±åº¦å­¦ä¹ è®­ç»ƒã€ç§‘å­¦è®¡ç®—

| å®ä¾‹ç±»å‹ | GPU | vCPU | å†…å­˜ | ç½‘ç»œæ€§èƒ½ |
|---------|-----|------|------|----------|
| p3.2xlarge | 1x V100 | 8 | 61 GB | æœ€é«˜ 10 Gbps |
| p3.8xlarge | 4x V100 | 32 | 244 GB | 10 Gbps |

## ğŸ’° æˆæœ¬ä¼˜åŒ–ç­–ç•¥

### Spot å®ä¾‹ä½¿ç”¨
```yaml
# åœ¨ NodePool ä¸­å¯ç”¨ Spot
requirements:
  - key: karpenter.sh/capacity-type
    operator: In
    values: ["spot"]  # ä»…ä½¿ç”¨ Spot å®ä¾‹
```

### è‡ªåŠ¨ç¼©å®¹é…ç½®
```yaml
# å¿«é€Ÿç¼©å®¹ä»¥èŠ‚çœæˆæœ¬
disruption:
  consolidationPolicy: WhenEmpty
  consolidateAfter: 30s  # 30ç§’åç¼©å®¹ç©ºé—²èŠ‚ç‚¹
```

### å®ä¾‹ç±»å‹ä¼˜å…ˆçº§
1. **å¼€å‘æµ‹è¯•**: g4dn.xlarge (Spot)
2. **ç”Ÿäº§æ¨ç†**: g5.xlarge (On-Demand)
3. **å¤§è§„æ¨¡è®­ç»ƒ**: p3.2xlarge (Spot + On-Demand æ··åˆ)

## ğŸ” ç›‘æ§å’Œæ•…éšœæ’é™¤

### æ£€æŸ¥ GPU èµ„æº
```bash
# æŸ¥çœ‹é›†ç¾¤ GPU èµ„æºæ€»é‡
kubectl describe nodes | grep -A 5 "Allocatable:" | grep nvidia.com/gpu

# æŸ¥çœ‹ GPU ä½¿ç”¨æƒ…å†µ
kubectl top nodes --selector=node-type=gpu
```

### å¸¸è§é—®é¢˜

#### 1. GPU èŠ‚ç‚¹æ— æ³•å¯åŠ¨
**æ£€æŸ¥**:
```bash
# æŸ¥çœ‹èŠ‚ç‚¹äº‹ä»¶
kubectl describe node <gpu-node-name>

# æ£€æŸ¥ Karpenter æ—¥å¿—
kubectl logs -n karpenter deployment/karpenter
```

#### 2. Device Plugin æ— æ³•è¿è¡Œ
**æ£€æŸ¥**:
```bash
# æŸ¥çœ‹ Device Plugin æ—¥å¿—
kubectl logs -n kube-system -l name=nvidia-device-plugin-ds

# éªŒè¯ NVIDIA é©±åŠ¨
kubectl exec -it <gpu-pod> -- nvidia-smi
```

#### 3. Pod æ— æ³•è°ƒåº¦åˆ° GPU èŠ‚ç‚¹
**æ£€æŸ¥**:
```bash
# ç¡®è®¤ Toleration å’Œ NodeSelector
kubectl describe pod <gpu-pod>

# æ£€æŸ¥èŠ‚ç‚¹æ±¡ç‚¹
kubectl describe node <gpu-node> | grep Taints
```

## ğŸ“š å‚è€ƒèµ„æ–™

- [EKS GPU å·¥ä½œè´Ÿè½½](https://docs.aws.amazon.com/eks/latest/userguide/gpu-ami.html)
- [Karpenter GPU æ”¯æŒ](https://karpenter.sh/docs/concepts/nodepools/)
- [NVIDIA Device Plugin](https://github.com/NVIDIA/k8s-device-plugin)
- [AWS Deep Learning AMI](https://docs.aws.amazon.com/dlami/latest/devguide/what-is-dlami.html)
