# PyTorch GPU 测试
apiVersion: v1
kind: Pod
metadata:
  name: gpu-pytorch-test
spec:
  containers:
  - name: pytorch-test
    image: pytorch/pytorch:latest
    command: 
    - python3
    - -c
    - |
      import torch
      print("=== PyTorch GPU Test Starting ===")
      print(f"PyTorch version: {torch.__version__}")
      print(f"CUDA available: {torch.cuda.is_available()}")
      if torch.cuda.is_available():
          print(f"CUDA version: {torch.version.cuda}")
          print(f"GPU count: {torch.cuda.device_count()}")
          print(f"GPU name: {torch.cuda.get_device_name(0)}")
          # 简单的 GPU 计算测试
          print("Starting GPU computation test...")
          x = torch.randn(1000, 1000).cuda()
          y = torch.randn(1000, 1000).cuda()
          z = torch.mm(x, y)
          print(f"GPU computation successful: {z.shape}")
          print("=== PyTorch GPU Test Completed Successfully ===")
      else:
          print("CUDA not available")
          print("=== PyTorch GPU Test Failed ===")

      # 保持容器运行，方便查看日志
      import time
      print("Keeping container alive for log inspection...")
      while True:
          time.sleep(600)  # 每10分钟打印一次状态
          print("Container still running...")
    resources:
      limits:
        nvidia.com/gpu: 1
      requests:
        nvidia.com/gpu: 1
  tolerations:
  - key: nvidia.com/gpu
    operator: Exists
    effect: NoSchedule
  nodeSelector:
    node-type: gpu
  restartPolicy: Always
