# AWS EKS é›†ç¾¤åˆ›å»ºæŒ‡å—

## å‰ç½®è¦æ±‚

### ç¯å¢ƒä¿¡æ¯
- AWS Profile: `lab`
- Region: `us-east-1` (æŒ‰éœ€é€‰æ‹©åŒºåŸŸ)
- é›†ç¾¤åç§°: `eks-karpenter-env`

### å¿…éœ€å·¥å…·ç‰ˆæœ¬
- AWS CLI v2.x
- eksctl >= 0.150.0
- kubectl >= 1.28
- helm >= 3.8
- ä½¿ç”¨ Karpenter v1.6.2 (ç¨³å®šç‰ˆæœ¬)
  - å®˜æ–¹ OCI ä»“åº“ï¼š`oci://public.ecr.aws/karpenter/karpenter`
  - API ç‰ˆæœ¬ï¼š`karpenter.sh/v1` å’Œ `karpenter.k8s.aws/v1`

## éƒ¨ç½²æ–¹æ¡ˆé€‰æ‹©

æœ¬é¡¹ç›®æä¾›ä¸¤ç§ç³»ç»Ÿç»„ä»¶éƒ¨ç½²æ–¹æ¡ˆï¼Œæ ¹æ®éœ€æ±‚é€‰æ‹©ï¼š

| å¯¹æ¯”é¡¹ | æ–¹æ¡ˆ A: Fargate | æ–¹æ¡ˆ B: NodeGroup |
|--------|----------------|-------------------|
| **é…ç½®æ–‡ä»¶** | `cluster-config.yaml` | `cluster-config-ngs.yaml` + `nodegroup-system.yaml` |
| **ç³»ç»Ÿç»„ä»¶è¿è¡Œä½ç½®** | Fargate (æ— æœåŠ¡å™¨) | Managed NodeGroup (EC2 Spot) |
| **è®¤è¯æ–¹å¼** | IRSA (Karpenter åœ¨ Fargate ä¸æ”¯æŒ Pod Identity) | Pod Identity (å…¨ç»„ä»¶ç»Ÿä¸€) |
| **èŠ‚ç‚¹ç®¡ç†** | æ— éœ€ç®¡ç† | è‡ªåŠ¨ä¼¸ç¼© (1-3 èŠ‚ç‚¹) |
| **è°ƒåº¦æ§åˆ¶** | éœ€è¦ `fargate: enabled` æ ‡ç­¾ | è‡ªåŠ¨è°ƒåº¦åˆ° system èŠ‚ç‚¹ç»„ |
| **å®ä¾‹æ¶æ„** | N/A (Fargate) | ARM64 Graviton Spot |
| **é€‚ç”¨åœºæ™¯** | å°è§„æ¨¡ã€å…è¿ç»´ | å¤§è§„æ¨¡ã€æˆæœ¬ä¼˜åŒ–ã€éœ€è¦ DaemonSet æ”¯æŒ |

### æ–¹æ¡ˆ A ç‰¹ç‚¹ (Fargate)
- ç³»ç»Ÿç»„ä»¶é€šè¿‡ Fargate Profile + `fargate: enabled` æ ‡ç­¾è°ƒåº¦
- Karpenter è¿è¡Œåœ¨ Fargateï¼Œéœ€ä½¿ç”¨ IRSAï¼ˆPod Identity Agent æ˜¯ DaemonSetï¼Œä¸æ”¯æŒ Fargateï¼‰
- LoadBalancer Controller ä½¿ç”¨ IRSA
- æ¯ä¸ª Pod ç‹¬ç«‹éš”ç¦»ï¼Œå®‰å…¨æ€§é«˜

### æ–¹æ¡ˆ B ç‰¹ç‚¹ (NodeGroup)
- ç³»ç»Ÿç»„ä»¶è¿è¡Œåœ¨ Managed NodeGroupï¼ˆSpot å®ä¾‹ï¼Œæˆæœ¬ä½ï¼‰
- æ‰€æœ‰ç»„ä»¶ç»Ÿä¸€ä½¿ç”¨ Pod Identityï¼Œé…ç½®æ›´ç®€æ´
- æ”¯æŒ DaemonSet ç±»å‹çš„ç³»ç»Ÿç»„ä»¶ï¼ˆå¦‚ CSI Nodeã€Pod Identity Agentï¼‰
- èŠ‚ç‚¹è‡ªåŠ¨ä¼¸ç¼© 1-3 å°ï¼Œå¤šå®ä¾‹ç±»å‹å®¹é”™

## 1: ç¯å¢ƒå‡†å¤‡

### 1.1 å®‰è£…å¿…è¦å·¥å…·

```bash
# å®‰è£… eksctl
curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
sudo mv /tmp/eksctl /usr/local/bin

# å®‰è£… kubectl
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
chmod +x kubectl
sudo mv kubectl /usr/local/bin/

# å®‰è£… helm
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

# éªŒè¯å®‰è£…
eksctl version
kubectl version --client
helm version
```

### 1.2 éªŒè¯ AWS é…ç½®

```bash
# éªŒè¯ AWS é…ç½®
aws sts get-caller-identity --profile lab
aws configure list --profile lab

# è®¾ç½®é»˜è®¤ profile (å¯é€‰)
export AWS_PROFILE=lab
export AWS_DEFAULT_REGION=us-east-1
```

## 2: EKS é›†ç¾¤åˆ›å»º

**é‡è¦è¯´æ˜**ï¼š
- å»ºè®®åˆ›å»ºä¹‹å‰é€šè¿‡ `eksctl create cluster --dry-run` è¿›è¡ŒéªŒè¯
- eksctl ä¸æ”¯æŒè‡ªåŠ¨æ·»åŠ  `karpenter.sh/discovery` æ ‡ç­¾
- éœ€è¦åœ¨é›†ç¾¤åˆ›å»ºåæ‰‹åŠ¨æ·»åŠ è¿™äº›æ ‡ç­¾ï¼ˆè§éªŒè¯æ­¥éª¤ï¼‰

### æ–¹æ¡ˆ A: Fargate æ–¹æ¡ˆéƒ¨ç½²

é…ç½®æ–‡ä»¶ï¼š`cluster-config.yaml`

```bash
# åˆ›å»ºé›†ç¾¤ (å¤§çº¦éœ€è¦ 15-20 åˆ†é’Ÿ)
eksctl create cluster -f cluster-config.yaml --profile lab
```

### æ–¹æ¡ˆ B: NodeGroup æ–¹æ¡ˆéƒ¨ç½²

é…ç½®æ–‡ä»¶ï¼š`cluster-config-ngs.yaml` + `nodegroup-system.yaml`

```bash
# ç¬¬ä¸€æ­¥ï¼šåˆ›å»ºé›†ç¾¤ï¼ˆä¸å«èŠ‚ç‚¹ç»„ï¼‰
eksctl create cluster -f cluster-config-ngs.yaml --profile lab

# ç¬¬äºŒæ­¥ï¼šåˆ›å»º system èŠ‚ç‚¹ç»„
eksctl create nodegroup -f nodegroup-system.yaml --profile lab
```

### é›†ç¾¤åˆ›å»ºåé€šç”¨æ­¥éª¤

```bash
# éªŒè¯é›†ç¾¤åˆ›å»º
kubectl get nodes
kubectl get pods -A

# ä¸º Karpenter æ·»åŠ å¿…è¦çš„èµ„æºæ ‡ç­¾
export CLUSTER_NAME=eks-karpenter-env
export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text --profile lab)

# ä¸ºç§æœ‰å­ç½‘æ·»åŠ  Karpenter å‘ç°æ ‡ç­¾
for subnet in $(aws ec2 describe-subnets --filters "Name=tag:aws:cloudformation:logical-id,Values=SubnetPrivate*" "Name=tag:aws:cloudformation:stack-name,Values=eksctl-${CLUSTER_NAME}-cluster" --profile lab --query 'Subnets[].SubnetId' --output text); do
  echo "æ·»åŠ æ ‡ç­¾åˆ°å­ç½‘: $subnet"
  aws ec2 create-tags --resources $subnet --tags Key=karpenter.sh/discovery,Value=${CLUSTER_NAME} --profile lab
done

# ä¸ºé›†ç¾¤å®‰å…¨ç»„æ·»åŠ  Karpenter å‘ç°æ ‡ç­¾
CLUSTER_SG=$(aws eks describe-cluster --name ${CLUSTER_NAME} --profile lab --query 'cluster.resourcesVpcConfig.clusterSecurityGroupId' --output text)
echo "æ·»åŠ æ ‡ç­¾åˆ°å®‰å…¨ç»„: $CLUSTER_SG"
aws ec2 create-tags --resources $CLUSTER_SG --tags Key=karpenter.sh/discovery,Value=${CLUSTER_NAME} --profile lab
```

#### ä»…æ–¹æ¡ˆ A éœ€è¦ï¼šæ›´æ–° aws-auth ConfigMap

```bash
# Fargate æ–¹æ¡ˆéœ€è¦æ‰‹åŠ¨æ·»åŠ  Karpenter èŠ‚ç‚¹è§’è‰²åˆ° aws-auth
FARGATE_ROLE=$(aws iam list-roles --profile lab --query 'Roles[?contains(RoleName, `FargatePodExecutionRole`)].Arn' --output text)
kubectl patch configmap aws-auth -n kube-system --patch "
data:
  mapRoles: |
    - groups:
      - system:bootstrappers
      - system:nodes
      - system:node-proxier
      rolearn: ${FARGATE_ROLE}
      username: system:node:{{SessionName}}
    - groups:
      - system:bootstrappers
      - system:nodes
      rolearn: arn:aws:iam::${AWS_ACCOUNT_ID}:role/KarpenterNodeInstanceRole-${CLUSTER_NAME}
      username: system:node:{{EC2PrivateDNSName}}
"
```

## 3: é›†ç¾¤é…ç½®

### 3.1 ç³»ç»Ÿç»„ä»¶è°ƒåº¦é…ç½®

#### æ–¹æ¡ˆ A: Fargate Profile é…ç½®

Fargate Profiles å·²åœ¨ `cluster-config.yaml` ä¸­é¢„é…ç½®ï¼Œeksctl ä¼šè‡ªåŠ¨åˆ›å»ºä»¥ä¸‹ Profilesï¼š

- **default** - ç”¨äº default å’Œ kube-system namespaceï¼ˆéœ€è¦ `fargate: enabled` æ ‡ç­¾ï¼‰
- **karpenter** - ç”¨äº karpenter namespaceï¼ˆéœ€è¦ `fargate: enabled` æ ‡ç­¾ï¼‰
- **portainer** - ç”¨äº portainer namespaceï¼ˆéœ€è¦ `fargate: enabled` æ ‡ç­¾ï¼‰

**ç²¾ç¡®æ ‡ç­¾æ§åˆ¶**ï¼š
- ä½¿ç”¨ `fargate: enabled` æ ‡ç­¾ç²¾ç¡®æ§åˆ¶å“ªäº› Pod è¿è¡Œåœ¨ Fargate
- é¿å…å…¨é‡æ•è·ï¼ˆæ— æ ‡ç­¾é€‰æ‹©å™¨ï¼‰ï¼Œé˜²æ­¢æ„å¤–çš„é«˜æˆæœ¬

**æ¶æ„è§„åˆ’**ï¼š
- **ç®¡ç†ç»„ä»¶** â†’ Fargateï¼ˆKarpenter, Portainer, Load Balancer Controllerï¼‰
- **ç³»ç»Ÿç»„ä»¶** â†’ EC2ï¼ˆCSI Controllers, CoreDNSï¼‰
- **åº”ç”¨è´Ÿè½½** â†’ EC2ï¼ˆé»˜è®¤ï¼‰

**éªŒè¯ Fargate Profiles**ï¼š
```bash
# æŸ¥çœ‹å·²åˆ›å»ºçš„ Fargate Profiles
aws eks list-fargate-profiles --cluster-name eks-karpenter-env --region us-east-1 --profile lab

# æŸ¥çœ‹å…·ä½“é…ç½®
aws eks describe-fargate-profile --cluster-name eks-karpenter-env --fargate-profile-name default --region us-east-1 --profile lab
```

**ä¸º addon æ·»åŠ  Fargate æ ‡ç­¾çš„æ–¹æ³•**ï¼š

1. **EKS Addon é…ç½®å‚æ•°**ï¼ˆæ¨èï¼‰ï¼š
```bash
aws eks create-addon \
  --cluster-name eks-karpenter-env \
  --addon-name aws-load-balancer-controller \
  --configuration-values '{"podLabels":{"fargate":"enabled"}}'
```

2. **å®‰è£…åç«‹å³ patch**ï¼ˆé€šç”¨ï¼‰ï¼š
```bash
kubectl patch deployment <addon-deployment-name> -n <namespace> \
  -p '{"spec":{"template":{"metadata":{"labels":{"fargate":"enabled"}}}}}'
```

3. **Helm values**ï¼ˆå¦‚æœä½¿ç”¨ Helmï¼‰ï¼š
```yaml
podLabels:
  fargate: enabled
```

#### æ–¹æ¡ˆ B: NodeGroup é…ç½®

System NodeGroup å·²åœ¨ `nodegroup-system.yaml` ä¸­é¢„é…ç½®ï¼š

- **èŠ‚ç‚¹ç»„åç§°**: `system`
- **å®ä¾‹ç±»å‹**: ARM64 Graviton (m8g/m7g/c8g/c7g/r8g/r7g, large/xlarge)
- **å®¹é‡**: æœ€å° 1 / æœŸæœ› 2 / æœ€å¤§ 3
- **èŠ‚ç‚¹æ ‡ç­¾**: `role: system`

ç³»ç»Ÿç»„ä»¶ï¼ˆCoreDNSã€CSI Controllersã€Karpenterã€LoadBalancer Controller ç­‰ï¼‰è‡ªåŠ¨è°ƒåº¦åˆ° system èŠ‚ç‚¹ç»„ï¼Œæ— éœ€é¢å¤–æ ‡ç­¾é…ç½®ã€‚

**éªŒè¯ NodeGroup**ï¼š
```bash
eksctl get nodegroup --cluster eks-karpenter-env --profile lab
kubectl get nodes -l role=system
```

### 3.2 IAM è®¤è¯é…ç½®

#### æ–¹æ¡ˆ A: IRSA + Pod Identity æ··åˆ

ç”±äº Karpenter è¿è¡Œåœ¨ Fargateï¼ŒPod Identity Agentï¼ˆDaemonSetï¼‰æ— æ³•è¿è¡Œåœ¨ Fargate èŠ‚ç‚¹ä¸Šï¼Œå› æ­¤ï¼š
- **Karpenter** â†’ IRSAï¼ˆé€šè¿‡ Helm `--set serviceAccount.annotations` é…ç½®ï¼‰
- **LoadBalancer Controller** â†’ IRSA
- **CSI Drivers** â†’ IRSA
- è¯¦è§ `cluster-config.yaml` ä¸­çš„ `iam.serviceAccounts` é…ç½®

#### æ–¹æ¡ˆ B: Pod Identity ç»Ÿä¸€

æ‰€æœ‰ç»„ä»¶ç»Ÿä¸€ä½¿ç”¨ Pod Identityï¼Œé…ç½®æ›´ç®€æ´ï¼š
- **æ‰€æœ‰ CSI Drivers** â†’ Pod Identity
- **LoadBalancer Controller** â†’ Pod Identity
- **Karpenter** â†’ Pod Identityï¼ˆè¿è¡Œåœ¨ EC2 èŠ‚ç‚¹ï¼Œæ”¯æŒ Pod Identity Agentï¼‰
- è¯¦è§ `cluster-config-ngs.yaml` ä¸­çš„ `iam.podIdentityAssociations` é…ç½®

**Pod Identity ä¼˜åŠ¿**ï¼ˆç›¸æ¯”ä¼ ç»Ÿ IRSAï¼‰ï¼š
- âœ… **æ— éœ€ç®¡ç† OIDC Provider** - è‡ªåŠ¨ç®¡ç†
- âœ… **ç®€åŒ– IAM ä¿¡ä»»ç­–ç•¥** - æ›´ç®€æ´çš„æƒé™é…ç½®
- âœ… **æ›´å¥½çš„è·¨è´¦æˆ·æ”¯æŒ** - ä¼ä¸šçº§æƒé™ç®¡ç†
- âœ… **æœªæ¥å…¼å®¹æ€§ä¿è¯** - AWS æ¨èçš„ç°ä»£æ–¹å¼

**éªŒè¯ Pod Identity é…ç½®**ï¼š
```bash
# æ£€æŸ¥ Pod Identity Agent çŠ¶æ€
aws eks describe-addon \
  --cluster-name eks-karpenter-env \
  --addon-name eks-pod-identity-agent \
  --region us-east-1 \
  --profile lab \
  --query '{Status:status,Version:addonVersion}'

# æŸ¥çœ‹ Pod Identity Associations
aws eks list-pod-identity-associations \
  --cluster-name eks-karpenter-env \
  --region us-east-1 \
  --profile lab
```

### 3.3 ä» IRSA è¿ç§»åˆ° Pod Identityï¼ˆç°æœ‰é›†ç¾¤ï¼‰

**æ³¨æ„**ï¼šå¦‚æœæ˜¯ç°æœ‰é›†ç¾¤éœ€è¦ä» IRSA è¿ç§»åˆ° Pod Identityï¼Œæ¨èä½¿ç”¨ migrate-to-pod-identity è¿ç§»å·¥å…·ï¼Œæ”¯æŒè‡ªåŠ¨å‘ç°éœ€è¦è¿ç§»çš„ addon å’ŒæœåŠ¡è´¦æˆ·ï¼Œè‡ªåŠ¨æ›´æ–° IAM è§’è‰²ä¿¡ä»»ç­–ç•¥ã€‚

**è¿ç§»æ­¥éª¤**ï¼š

1. **é¢„è§ˆè¿ç§»è®¡åˆ’**ï¼š
```bash
# æŸ¥çœ‹å“ªäº›ç»„ä»¶å¯ä»¥è¿ç§»
eksctl utils migrate-to-pod-identity \
  --cluster eks-karpenter-env \
  --region ap-southeast-1 \
  --profile lab
```

2. **æ‰§è¡Œè¿ç§»**ï¼š
```bash
# æ‰§è¡Œè¿ç§»å¹¶ç§»é™¤ OIDC ä¿¡ä»»å…³ç³»
eksctl utils migrate-to-pod-identity \
  --cluster eks-karpenter-env \
  --region ap-southeast-1 \
  --profile lab \
  --approve \
  --remove-oidc-provider-trust-relationship
```

3. **é‡å¯ç›¸å…³æœåŠ¡**ï¼š
```bash
# é‡å¯è¿ç§»çš„ç»„ä»¶ä½¿å…¶ä½¿ç”¨æ–°çš„ Pod Identity
kubectl rollout restart daemonset/aws-node -n kube-system
kubectl rollout restart deployment/ebs-csi-controller -n kube-system
kubectl rollout restart deployment/efs-csi-controller -n kube-system
```

**éªŒè¯è¿ç§»ç»“æœ**ï¼š
```bash
# æ£€æŸ¥ Pod Identity å…³è”
aws eks list-pod-identity-associations \
  --cluster-name eks-karpenter-env \
  --region ap-southeast-1 \
  --profile lab

# éªŒè¯æœåŠ¡è´¦æˆ·ä¸å†æœ‰ IRSA æ³¨è§£
kubectl get serviceaccount -n kube-system aws-node -o yaml | grep -i role-arn
```

## 4: å­˜å‚¨é…ç½®

**é€šè¿‡ cluster-config.yaml è‡ªåŠ¨å®‰è£…çš„ CSI Drivers ç»„ä»¶**ï¼š
1. **EBS CSI Driver** - å·²é€šè¿‡ addon è‡ªåŠ¨å®‰è£…ï¼ŒåŒ…å« IAM æƒé™
2. **EFS CSI Driver** - å·²é€šè¿‡ addon è‡ªåŠ¨å®‰è£…ï¼ŒåŒ…å« IAM æƒé™
3. **S3 CSI Driver** - å·²é€šè¿‡ addon è‡ªåŠ¨å®‰è£…ï¼ŒåŒ…å« IAM æƒé™

**CSI Controller è°ƒåº¦ç­–ç•¥**ï¼š
- âœ… **CSI Controller** â†’ è‡ªåŠ¨è¿è¡Œåœ¨ EC2 èŠ‚ç‚¹ï¼ˆéœ€è¦ç‰¹æƒå®¹å™¨ï¼‰
- âœ… **CSI Node** â†’ å¯ä»¥è¿è¡Œåœ¨ä»»ä½•èŠ‚ç‚¹ï¼ˆåŒ…æ‹¬ Fargateï¼‰
- âœ… **ç®¡ç†ç»„ä»¶** â†’ è¿è¡Œåœ¨ Fargateï¼ˆæ·»åŠ  `fargate: enabled` æ ‡ç­¾ï¼‰

### 4.1 åˆ›å»º EFS æ–‡ä»¶ç³»ç»Ÿ

```bash
# è·å– VPC ID
VPC_ID=$(aws eks describe-cluster --name eks-karpenter-env --query "cluster.resourcesVpcConfig.vpcId" --output text --profile lab)

# è·å– CIDR å—
CIDR_BLOCK=$(aws ec2 describe-vpcs --vpc-ids $VPC_ID --query "Vpcs[0].CidrBlock" --output text --profile lab)

# åˆ›å»ºå®‰å…¨ç»„
SECURITY_GROUP_ID=$(aws ec2 create-security-group \
  --group-name EFS-SecurityGroup-eks-karpenter-env \
  --description "Security group for EFS mount targets" \
  --vpc-id $VPC_ID \
  --output text \
  --query 'GroupId' \
  --profile lab)

# æ·»åŠ  NFS å…¥ç«™è§„åˆ™
aws ec2 authorize-security-group-ingress \
  --group-id $SECURITY_GROUP_ID \
  --protocol tcp \
  --port 2049 \
  --cidr $CIDR_BLOCK \
  --profile lab

# åˆ›å»º EFS æ–‡ä»¶ç³»ç»Ÿ
EFS_ID=$(aws efs create-file-system \
  --creation-token eks-karpenter-env-efs \
  --performance-mode generalPurpose \
  --throughput-mode provisioned \
  --provisioned-throughput-in-mibps 100 \
  --encrypted \
  --output text \
  --query 'FileSystemId' \
  --profile lab)

echo "EFS File System ID: $EFS_ID"

# è·å–å­ç½‘ ID
SUBNET_IDS=$(aws ec2 describe-subnets \
  --filters "Name=vpc-id,Values=$VPC_ID" "Name=availability-zone,Values=ap-southeast-1a,ap-southeast-1b,ap-southeast-1c" \
  --query 'Subnets[?MapPublicIpOnLaunch==`false`].SubnetId' \
  --output text \
  --profile lab)

# ä¸ºæ¯ä¸ªå­ç½‘åˆ›å»ºæŒ‚è½½ç›®æ ‡
for subnet in $SUBNET_IDS; do
  aws efs create-mount-target \
    --file-system-id $EFS_ID \
    --subnet-id $subnet \
    --security-groups $SECURITY_GROUP_ID \
    --profile lab
done
```

### 4.2 åˆ›å»ºå­˜å‚¨ç±»é…ç½®

**æ³¨æ„**ï¼šé¡¹ç›®ä¸­å·²åŒ…å«é¢„é…ç½®çš„å­˜å‚¨ç±»æ–‡ä»¶ï¼Œéœ€è¦æ›´æ–° EFS ID ååº”ç”¨ã€‚

```bash
# æ›´æ–° general-storageclasses.yaml ä¸­çš„ EFS IDï¼ˆä½¿ç”¨ä¸Šä¸€æ­¥åˆ›å»ºçš„ EFS IDï¼‰
sed -i "s/fs-0123456789abcdef0/$EFS_ID/g" eks/general-storageclasses.yaml

# åº”ç”¨å­˜å‚¨ç±»é…ç½®
kubectl apply -f eks/general-storageclasses.yaml

# éªŒè¯å­˜å‚¨ç±»
kubectl get storageclass
```

### 4.3 éªŒè¯ S3 CSI Driver

**âœ… è‡ªåŠ¨å®‰è£…**ï¼šS3 CSI Driver å·²åœ¨ cluster-config.yaml ä¸­é…ç½®ä¸ºè‡ªåŠ¨å®‰è£…çš„ EKS Addonã€‚

**éªŒè¯å®‰è£…**ï¼š
```bash
# æ£€æŸ¥ S3 CSI Driver Addon çŠ¶æ€
aws eks describe-addon \
  --cluster-name eks-karpenter-env \
  --addon-name aws-mountpoint-s3-csi-driver \
  --profile lab \
  --query 'addon.{Status:status,Version:addonVersion}'

# éªŒè¯ S3 CSI Driver Pod
kubectl get pods -n kube-system -l app=s3-csi-node

# éªŒè¯ CSI Driver æ³¨å†Œ
kubectl get csidriver s3.csi.aws.com
```

### 4.4 åˆ›å»ºæµ‹è¯• S3 å­˜å‚¨æ¡¶

```bash
# åˆ›å»º S3 å­˜å‚¨æ¡¶
BUCKET_NAME="eks-karpenter-env-storage-$(date +%s)"
aws s3 mb s3://$BUCKET_NAME --region us-east-1 --profile lab

echo "S3 Bucket: $BUCKET_NAME"
```

## 5: AWS LoadBalancer Controller å®‰è£…

**ä½¿ç”¨ LoadBalancer Controller çš„ä¼˜åŠ¿**ï¼š
- âœ… **ç°ä»£åŒ–** - ä½¿ç”¨ ALB æ›¿ä»£å³å°†å¼ƒç”¨çš„ Classic Load Balancer
- ğŸ’° **æˆæœ¬æ•ˆç‡** - ALB æ¯” Classic Load Balancer æ›´ç»æµ
- ğŸš€ **åŠŸèƒ½ä¸°å¯Œ** - æ”¯æŒè·¯å¾„è·¯ç”±ã€SSL ç»ˆæ­¢ã€WAF é›†æˆç­‰
- ğŸ“Š **æ›´å¥½ç›‘æ§** - é›†æˆ CloudWatch æŒ‡æ ‡å’Œæ—¥å¿—

### 5.1 å®‰è£… AWS LoadBalancer Controller

**æ³¨æ„**ï¼šæœåŠ¡è´¦æˆ·å·²åœ¨é›†ç¾¤é…ç½®æ–‡ä»¶ä¸­è‡ªåŠ¨åˆ›å»ºï¼ŒåŒ…å«æ‰€éœ€çš„ IAM æƒé™ã€‚

```bash
# 1. æ·»åŠ  EKS Helm ä»“åº“
helm repo add eks https://aws.github.io/eks-charts
helm repo update

# 2. è·å– VPC ID
VPC_ID=$(aws eks describe-cluster --name eks-karpenter-env --query "cluster.resourcesVpcConfig.vpcId" --output text --profile lab)
```

#### æ–¹æ¡ˆ A (Fargate): éœ€è¦æ·»åŠ  Fargate æ ‡ç­¾

```bash
helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
  -n kube-system \
  --set clusterName=eks-karpenter-env \
  --set serviceAccount.create=false \
  --set serviceAccount.name=aws-load-balancer-controller \
  --set vpcId=$VPC_ID \
  --set region=us-east-1 \
  --set podLabels.fargate=enabled
```

#### æ–¹æ¡ˆ B (NodeGroup): æ— éœ€é¢å¤–æ ‡ç­¾

```bash
helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
  -n kube-system \
  --set clusterName=eks-karpenter-env \
  --set serviceAccount.create=false \
  --set serviceAccount.name=aws-load-balancer-controller \
  --set vpcId=$VPC_ID \
  --set region=us-east-1
```

#### éªŒè¯å®‰è£…

```bash
kubectl get deployment -n kube-system aws-load-balancer-controller
kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller
```

### 5.2 ä½¿ç”¨ ALB Ingress æ›¿ä»£ LoadBalancer Service

å®‰è£…å®Œæˆåï¼Œå¯ä»¥ä½¿ç”¨ ALB Ingress æ›¿ä»£ä¼ ç»Ÿçš„ LoadBalancer Serviceï¼ˆé»˜è®¤åˆ›å»º Classic Load Balancerï¼‰ã€‚

```yaml
# ç¤ºä¾‹ï¼šALB Ingress é…ç½®
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: example-ingress
  annotations:
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTP":80}]'
spec:
  ingressClassName: alb
  rules:
  - http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: example-service
            port:
              number: 80
```

### 5.3 Gateway API CRD æ›´æ–°ï¼ˆå¯é€‰ï¼‰

å¦‚æœä½¿ç”¨ Gateway API åŠŸèƒ½ï¼Œéœ€è¦æ›´æ–° CRDï¼š

```bash
# æ›´æ–° Gateway API CRDsï¼ˆä»…åœ¨ä½¿ç”¨ Gateway API æ—¶éœ€è¦ï¼‰
kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/config/crd/gateway/gateway-crds.yaml

# éªŒè¯ CRD æ›´æ–°
kubectl get crd | grep gateway
```

## 6: éªŒè¯å’Œæµ‹è¯•

### 6.1 æµ‹è¯• ALB Ingress

```bash
# åº”ç”¨ ALB Ingress æµ‹è¯•é…ç½®
kubectl apply -f ../tests/test-alb-ingress.yaml

# ç­‰å¾… ALB åˆ›å»ºå®Œæˆ
kubectl get ingress test-alb-ingress -w

# è·å– ALB åœ°å€
ALB_URL=$(kubectl get ingress test-alb-ingress -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
echo "ALB URL: http://$ALB_URL"

# æµ‹è¯•è®¿é—®
curl http://$ALB_URL
```

### 6.2 æµ‹è¯• EFS å­˜å‚¨

```bash
# åº”ç”¨ EFS å­˜å‚¨æµ‹è¯•é…ç½®
kubectl apply -f ../tests/test-storage-efs.yaml

# éªŒè¯ PVC çŠ¶æ€
kubectl get pvc efs-pvc

# æŸ¥çœ‹æµ‹è¯• Pod æ—¥å¿—
kubectl logs -f efs-test-pod

# éªŒè¯æ•°æ®æŒä¹…åŒ–
kubectl exec efs-test-pod -- cat /mnt/efs/test.txt
```

### 6.3 æµ‹è¯• S3 æŒ‚è½½

```bash
# æ›´æ–° S3 å­˜å‚¨æ¡¶åç§°
sed -i "s/eks-karpenter-env-storage-1234567890/$BUCKET_NAME/g" ../tests/test-storage-s3.yaml

# åº”ç”¨ S3 å­˜å‚¨æµ‹è¯•é…ç½®ï¼ˆä½¿ç”¨ PV + PVC æ–¹å¼ï¼‰
kubectl apply -f ../tests/test-storage-s3.yaml

# éªŒè¯ PV å’Œ PVC çŠ¶æ€
kubectl get pv s3-pv
kubectl get pvc s3-claim

# æŸ¥çœ‹æµ‹è¯• Pod çŠ¶æ€å’Œæ—¥å¿—
kubectl get pod s3-test-pod
kubectl logs s3-test-pod --tail=5

# éªŒè¯ S3 æ•°æ®åŒæ­¥
aws s3 ls s3://$BUCKET_NAME/ --profile lab
kubectl exec s3-test-pod -- cat /mnt/s3/test.txt
```

### 6.4 æ¸…ç†æµ‹è¯•èµ„æº

```bash
# æ¸…ç†æµ‹è¯•èµ„æº
kubectl delete -f ../tests/test-storage-efs.yaml
kubectl delete -f ../tests/test-storage-s3.yaml
kubectl delete -f ../tests/test-alb-ingress.yaml
## æ¸…ç†èµ„æº

### æ¸…ç†æµ‹è¯•èµ„æº
```bash
# æ¸…ç†æµ‹è¯•èµ„æº
kubectl delete -f ../tests/test-storage-efs.yaml
kubectl delete -f ../tests/test-storage-s3.yaml  
kubectl delete -f ../tests/test-alb-ingress.yaml
```

### 6.5 å®Œå…¨æ¸…ç†é›†ç¾¤
```bash
# åˆ é™¤é›†ç¾¤ï¼ˆä¼šè‡ªåŠ¨æ¸…ç†å¤§éƒ¨åˆ†èµ„æºï¼‰
eksctl delete cluster --name eks-karpenter-env --profile lab

# æ‰‹åŠ¨æ¸…ç†æ®‹ç•™èµ„æº
# åˆ é™¤ S3 å­˜å‚¨æ¡¶
aws s3 rb s3://$BUCKET_NAME --force --profile lab

# åˆ é™¤ EFS æ–‡ä»¶ç³»ç»Ÿï¼ˆå¯é€‰ï¼Œå¦‚æœè¦ä¿ç•™æ•°æ®å¯è·³è¿‡ï¼‰
aws efs delete-file-system --file-system-id $EFS_ID --profile lab
```

## æ³¨æ„äº‹é¡¹

1. ç¡®ä¿ AWS è´¦æˆ·æœ‰è¶³å¤Ÿçš„æœåŠ¡é™é¢
2. ç¡®ä¿éƒ¨ç½²åŒºåŸŸçš„å®ä¾‹ç±»å‹å¯ç”¨æ€§
3. EFS å’Œ S3 çš„æˆæœ¬éœ€è¦è€ƒè™‘åœ¨å†…
4. å®šæœŸæ£€æŸ¥å’Œæ›´æ–°ç»„ä»¶ç‰ˆæœ¬
5. ç›‘æ§é›†ç¾¤èµ„æºä½¿ç”¨æƒ…å†µå’Œæˆæœ¬
